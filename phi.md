---
layout: default
title: Phi
permalink: /phi/
---

# A story of SLM: Roadmap of Phi Model Family

## Phi-4 Family: Moving forward and more on reasoning
* **\[05/192025\]** [Phi-4-mini](https://huggingface.co/microsoft/Phi-4-mini-instruct), as a small language model that excels at text-based tasks, providing high accuracy in a compact form, has been built into Edge browser for Prompt API and Writing Assistance APIs. Blog <span>&#8594;</span> [Simplified access to AI in Microsoft Edge](https://blogs.windows.com/msedgedev/2025/05/19/introducing-the-prompt-and-writing-assistance-apis/?user_id=67f3f84f6abb76033cd591d4&sn_type=LINKEDIN&cpost_id=682b9af719eb291191dc80fa&post_id=100007940800559&asset_id=ADVOCACY_205_682b97c1f930a4013428bd03)
* **\[05/01/2025\]** Celebrations on one-year milestone of Phi with Phi-reasoning versions! Blog <span>&#8594;</span> [One year of Phi: Small language models making big leaps in AI](https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/) and [technical report](https://arxiv.org/abs/2504.21233). 
* **\[02/26/2025\]** Try the latest members in the Phi family: <span>&#x1F917;</span>[Phi-4-mini](https://huggingface.co/microsoft/Phi-4-mini-instruct) and <span>&#x1F917;</span>[Phi-4-multimodal](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)! Read more <span>&#8594;</span> [Empowering innovation: The next generation of the Phi family](https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/) and our [techinical report](https://arxiv.org/abs/2503.01743).

**Phi-4 & Reasoning Series**

| | Phi-4 Models |  Phi-4-mini Models | Phi-4-multimodal Model |
|:------- |:-------|:-------|:-------|
| Base | <span>&#x1F917;</span>[Phi-4](https://huggingface.co/microsoft/phi-4) | <span>&#x1F917;</span>[Phi-4-mini](https://huggingface.co/microsoft/Phi-4-mini-instruct) | <span>&#x1F917;</span>[Phi-4-multimodal](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) | 
| Reasoning | <span>&#x1F917;</span>[Phi-4-reasoning](https://huggingface.co/microsoft/Phi-4-reasoning), <span>&#x1F917;</span>[Phi-4-reasoning-plus](https://huggingface.co/microsoft/Phi-4-reasoning-plus) |  <span>&#x1F917;</span>[Phi-4-mini-reasoning](https://huggingface.co/microsoft/Phi-4-mini-reasoning) |  |

## Phi-Silica: On-device SLM on Windows 11 Copilot+ PCs
* **\[04/25/2025\]** Expanding on the breakthrough efficiencies of [Phi Silica](https://learn.microsoft.com/en-us/windows/ai/apis/phi-silica), vision-based multimodal capabilities are added to unlock new possibilities for local SLMs on Windows. Blog <span>&#8594;</span> [Enabling multimodal functionality for Phi Silica](https://blogs.windows.com/windowsexperience/2025/04/25/enabling-multimodal-functionality-for-phi-silica/)
* **\[12/6/2024\]** Based on a Cyber-EO compliant derivative of Phi-3.5-mini, Phi-Silica is developed specifically for Windows 11 with multilingual support and on-device rewrite and summarize support in Word and Outlook. Specially about one of the post-training steps for alignment on safety and responsible AI, Phi Silica is derived has undergone a five stage ‘break-fix’ methodology similar to the one outlined in our technical report [Phi-3 Safety Post-Training: Aligning Language Models with a “Break-Fix” Cycle](https://arxiv.org/abs/2407.13833). Blog <span>&#8594;</span> [Phi Silica, small but mighty on-device SLM](https://blogs.windows.com/windowsexperience/2024/12/06/phi-silica-small-but-mighty-on-device-slm/).

## Phi-3 Family: Various sizes and context lengths, improved Phi-3.5 with MoE and vision

* **\[08/22/2024\]** Welcome <span>&#x1F917;</span>[Phi-3.5-mini](https://huggingface.co/microsoft/Phi-3.5-mini-instruct), Phi-3.5-vision, Phi-3.5-MoE, latest release as Phi-3.5 series SLMs! Read more <span>&#8594;</span> [Discover the New Multi-Lingual, High-Quality Phi-3.5 SLMs](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/4225280) and our latest [technical report](https://export.arxiv.org/abs/2404.14219).

**Phi-3.5 Series**

| Base |  MoE  |     Vision |
|:-------|:-------| :------------------------|
| <span>&#x1F917;</span>[Phi-3.5-mini](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) | <span>&#x1F917;</span>[Phi-3.5-MoE](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) | <span>&#x1F917;</span>[Phi-3.5-vision](https://huggingface.co/microsoft/Phi-3.5-vision-instruct) |

* **\[04/23/2024\]** Today we're launching the first publicly available small language model from our [Phi-3 family](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3) of open models, coming in mini, small and medium three sizes and 4k/8k and 128k context lengths. Try start with the most popular (top download) [Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and [Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)! Read more <span>&#8594;</span> [Tiny but mighty: The Phi-3 small language models with big potential](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/?ocid=FY24_soc_omc_br_li_Phi3)

**Phi-3 Series**

| Name |  Model  |           Model (Long-Context) |
|:-------|:-------| :------------------------|
| Mini (3.8B) | <span>&#x1F917;</span>[Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) | <span>&#x1F917;</span>[Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) | 
| Small (7B) | <span>&#x1F917;</span>[Phi-3-small-8k-instruct](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) | <span>&#x1F917;</span>[Phi-3-small-128k-instruct](https://huggingface.co/microsoft/Phi-3-small-128k-instruct) | 
| Medium (14B) |<span>&#x1F917;</span>[Phi-3-medium-4k-instruct](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct)  | <span>&#x1F917;</span>[Phi-3-medium-128k-instruct](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct) | 

## Phi-1 Family: Era of small language models, MVP for high-quality textbook data
* **\[10/02/2023\]** Updated version of [Phi-1.5](https://huggingface.co/microsoft/phi-1_5) was released, trained with augmented with a new data source that consists of various NLP synthetic texts. Read the technical report here: [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)
* **\[06/20/2023\]** The first Phi model named <span>&#x1F917;</span>[Phi-1](https://huggingface.co/microsoft/phi-1) with 1.3B parameters was released. 
