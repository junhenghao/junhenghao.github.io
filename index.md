---
#
# By default, content added below the "---" mark will appear in the home page
# between the top bar and the list of recent posts.
# To change the home page layout, edit the _layouts/home.html file.
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
#
layout: default
title: Junheng Hao
---
<img src="/assets/img/newself.jpg" alt="Profile Picture" width="200">

I am currently a researcher at Microsoft, working on LLM ([Phi-3](/phi), GPT, etc) training and customization. Prior to this, I obtained my Ph.D from University of California Los Angeles (UCLA), advised by [Yizhou Sun](http://web.cs.ucla.edu/~yzsun/) and [Wei Wang](https://web.cs.ucla.edu/~weiwang/) at [Scalable Analytics Institute (ScAi)](https://scai.cs.ucla.edu/) and [UCLA Data Mining Lab](https://ucla-dm.github.io/DM_website/index.html) in Department of Computer Science. Before coming to UCLA, I graduated in 2017 from Department of [Automation](https://www.au.tsinghua.edu.cn/), School of Information Science and Technology, [Tsinghua University](https://www.tsinghua.edu.cn/en/). More about me: [Resume](/assets/files/junheng_hao_resume.pdf)/[CV](/assets/files/junheng_hao_cv.pdf) 

<!-- <p>You can find more information about my <a href="/publication/">publications</a> and <a href="/teaching/">projects</a>.  -->

## Education
* Ph.D. in Computer Science, University of California Los Angeles (UCLA), 2022
* B.Eng. in School of Information Science and Technology, Tsinghua University, 2017

## Experiences
* Reseacher, **Microsoft GenAI**, Oct 2022 - Present
* Research Intern, **Microsoft Research (MSR)**, Jun 2021 - Sep 2021
* PhD Research Intern, **IBM Research AI**, Jun 2020 - Sep 2020
* Applied Scientist Intern, **Amazon**, Jun 2019 - Dec 2019
* Research Intern, **NEC Lab America**, Jun 2018 - Sep 2018

## Research Interests
* Large language model ([Phi](/phi), GPT-4o) Training: Pre-training, Post-training, RLHF, etc
* Systematic data strategies for LLM: data selection and synthetic data generation
* Customized LLM development: Domain-specific LLM, Reasoning/Coding
* LLM + Knowledge Graph (KG)
* LLM benchmarking and evaluation

## News & Blogs 
* \[05/01/2025\] Celebrations on one-year milestone of Phi! Blog <span>&#8594;</span> [One year of Phi: Small language models making big leaps in AI](https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/)
* \[02/26/2025\] Try the latest members in the Phi family: <span>&#x1F917;</span>[Phi-4-mini](https://huggingface.co/microsoft/Phi-4-mini-instruct) and <span>&#x1F917;</span>[Phi-4-multimodal](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)! Read more: [Empowering innovation: The next generation of the Phi family](https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/) and our [techinical report](https://arxiv.org/abs/2503.01743).
* \[10/01/2024\] A new collaboration and milestone for customized AI model to accelerate world-wide AI innovation with [KT](https://news.microsoft.com/source/2024/09/29/kt-corporation-and-microsoft-take-giant-step-to-accelerate-ai-innovation-in-korea/) and [G42](https://news.microsoft.com/source/2024/09/17/another-important-step-in-advancing-responsible-ai-to-serve-the-world/)! 
* \[08/22/2024\] Welcome <span>&#x1F917;</span>[Phi-3.5-mini](https://huggingface.co/microsoft/Phi-3.5-mini-instruct), Phi-3.5-vision, Phi-3.5-MoE, latest release as Phi-3.5 series SLMs! Read more: [Discover the New Multi-Lingual, High-Quality Phi-3.5 SLMs](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/4225280) and our latest [technical report](https://export.arxiv.org/abs/2404.14219).
* \[04/23/2024\] Today we're launching the first publicly available small language model from our [Phi-3 family](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3) of open models, coming in mini, small and medium three sizes and 4k/8k and 128k context lengths. Try start with <span>&#x1F917;</span>[Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and <span>&#x1F917;</span>[Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)! Read more: [Tiny but mighty: The Phi-3 small language models with big potential](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/?ocid=FY24_soc_omc_br_li_Phi3)
* More stories about the [roadmap](/phi) of [Phi model collection](https://ai.azure.com/explore/models?selectedCollection=phi)! 


## Selected Publications
Check full list here at [Publications](/publication) and [Google Scholar](https://scholar.google.com/citations?user=GL1yyoEAAAAJ&hl=en)

* Microsoft GenAI Team. [Phi-4-mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs](https://arxiv.org/abs/2503.01743), 2025. Read more: [Phi-4-Mini Model Release](https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/)

* Microsoft GenAI Team. [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://export.arxiv.org/abs/2404.14219), 2024. Read more: [Blog: Tiny but mighty: The Phi-3 small language models with big potential](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/?ocid=FY24_soc_omc_br_li_Phi3) [Phi-3 Model Release](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/)

* Yubo Ma, **Junheng Hao**, Ruochen Xu, Shuohang Wang, Zhibin Gou, Liangming Pan, Yujiu Yang, Yixin Cao, Aixin Sun, Hany Hassan Awadalla, Weizhu Chen. [SciAgent: A Tool-augmented LLM for Scientific Reasoning](https://arxiv.org/abs/2402.11451) The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)

* Jiazhan Feng, Ruochen Xu, **Junheng Hao**, Hiteshi Sharma, Dongyan Zhao. [Language Models can be Logical Solvers](https://arxiv.org/abs/2311.06158) 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)

* **Junheng Hao**, Tong Zhao, Jin Li, Xin Luna Dong, Christos Faloutsos, Yizhou Sun and Wei Wang. "[P-Companion: A Principled Framework for Diversified Complementary Product Recommendation](https://dl.acm.org/doi/10.1145/3340531.3412732)". In the 29th ACM International Conference on Information and Knowledge Management (CIKM 2020), Applied Research Track. [\[Amazon Blog\]](https://www.amazon.science/blog/improving-complementary-product-recommendations)

* **Junheng Hao**, Chelsea J.-T. Ju, Muhao Chen, Yizhou Sun, Carlo Zaniolo and Wei Wang. "[Bio-JOIE: Joint Representation Learning of Biological Knowledge Bases](https://dl.acm.org/doi/10.1145/3388440.3412477)". In the 11th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB 2020, <span style="color:red"> Best Student Paper Award</span>). [\[UCLA CS News\]](https://www.cs.ucla.edu/paper-from-ucla-scalable-analytics-institute-wins-best-student-paper-award-at-acm-bcb-2020/) 

* **Junheng Hao**, Muhao Chen, Wenchao Yu, Yizhou Sun, Wei Wang. "[Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Concepts](https://dl.acm.org/citation.cfm?id=3330838)". In the 25th International ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2019.

## Contact
Email: [junhenghao@microsoft.com]((mailto:junhenghao@microsoft.com)) (Microsoft) [haojh.ucla@gmail.com](mailto:haojh.ucla@gmail.com) (Personal)
<!-- <p>Feel free to <a href="/contact/">contact me</a>.</p> -->
